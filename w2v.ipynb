{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  1,\n       11,  2,  2,  0,  1,  1])"
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "past_values = np.load('dataset_train/past_values_da.npy')\n",
    "vocabulary = np.arange(24)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [],
   "source": [
    "class EarlyStopping():\n",
    "    def __init__(self, patience=5, min_percent_gain=0.1):\n",
    "        self.patience = patience\n",
    "        self.loss_list = []\n",
    "        self.min_percent_gain = min_percent_gain / 100.\n",
    "\n",
    "    def update_loss(self, loss):\n",
    "        self.loss_list.append(loss)\n",
    "        if len(self.loss_list) > self.patience:\n",
    "            del self.loss_list[0]\n",
    "\n",
    "    def stop_training(self):\n",
    "        if len(self.loss_list) == 1:\n",
    "            return False\n",
    "        gain = (max(self.loss_list) - min(self.loss_list)) / max(self.loss_list)\n",
    "        print(\"Loss gain: {}%\".format(round(100*gain,2)))\n",
    "        if gain < self.min_percent_gain:\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [],
   "source": [
    "def get_batches(context_tuple_list, batch_size=100):\n",
    "    random.shuffle(context_tuple_list)\n",
    "    batches = []\n",
    "    batch_target, batch_context, batch_negative = [], [], []\n",
    "    for i in range(len(context_tuple_list)):\n",
    "        batch_target.append(context_tuple_list[i][0])\n",
    "        batch_context.append(context_tuple_list[i][1])\n",
    "        batch_negative.append([w for w in context_tuple_list[i][2]])\n",
    "        if (i+1) % batch_size == 0 or i == len(context_tuple_list)-1:\n",
    "            tensor_target = autograd.Variable(torch.from_numpy(np.array(batch_target)).long())\n",
    "            tensor_context = autograd.Variable(torch.from_numpy(np.array(batch_context)).long())\n",
    "            tensor_negative = autograd.Variable(torch.from_numpy(np.array(batch_negative)).long())\n",
    "            batches.append((tensor_target, tensor_context, tensor_negative))\n",
    "            batch_target, batch_context, batch_negative = [], [], []\n",
    "    return batches"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [],
   "source": [
    "from numpy.random import multinomial\n",
    "from collections import Counter\n",
    "import random, math\n",
    "import re\n",
    "import itertools\n",
    "def sample_negative(sample_size):\n",
    "    sample_probability = {}\n",
    "    word_counts = dict(Counter(list(itertools.chain.from_iterable(past_values))))\n",
    "    normalizing_factor = sum([v**0.75 for v in word_counts.values()])\n",
    "    for word in word_counts:\n",
    "        sample_probability[word] = word_counts[word]**0.75 / normalizing_factor\n",
    "    words = np.array(list(word_counts.keys()))\n",
    "    while True:\n",
    "        word_list = []\n",
    "        sampled_index = np.array(multinomial(sample_size, list(sample_probability.values())))\n",
    "        for index, count in enumerate(sampled_index):\n",
    "            for _ in range(count):\n",
    "                 word_list.append(words[index])\n",
    "        yield word_list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 626980 pairs of target and context words\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "context_tuple_list = []\n",
    "\n",
    "negative_samples = sample_negative(8)\n",
    "\n",
    "for text in past_values:\n",
    "    for i, word in enumerate(text):\n",
    "        first_context_word_index = max(0,i-w)\n",
    "        last_context_word_index = min(i+w, len(text))\n",
    "        for j in range(first_context_word_index, last_context_word_index):\n",
    "            if i!=j:\n",
    "                context_tuple_list.append((word, text[j], next(negative_samples)))\n",
    "print(\"There are {} pairs of target and context words\".format(len(context_tuple_list)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Word2Vec(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_size, vocab_size):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.embeddings_target = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.embeddings_context = nn.Embedding(vocab_size, embedding_size)\n",
    "\n",
    "    def forward(self, target_word, context_word, negative_example):\n",
    "        emb_target = self.embeddings_target(target_word)\n",
    "        emb_context = self.embeddings_context(context_word)\n",
    "        emb_product = torch.mul(emb_target, emb_context)\n",
    "        emb_product = torch.sum(emb_product, dim=1)\n",
    "        out = torch.sum(F.logsigmoid(emb_product))\n",
    "        emb_negative = self.embeddings_context(negative_example)\n",
    "        emb_product = torch.bmm(emb_negative, emb_target.unsqueeze(2))\n",
    "        emb_product = torch.sum(emb_product, dim=1)\n",
    "        out += torch.sum(F.logsigmoid(-emb_product))\n",
    "        return -out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  1704.3027\n",
      "Loss gain: 0.19%\n",
      "Loss:  1703.8567\n",
      "Loss gain: 0.15%\n",
      "Loss:  1703.4968\n",
      "Loss gain: 0.12%\n",
      "Loss:  1703.1843\n",
      "Loss gain: 0.1%\n",
      "Loss:  1702.9463\n",
      "Loss gain: 0.08%\n",
      "Loss:  1702.6953\n",
      "Loss gain: 0.07%\n",
      "Loss:  1702.5277\n",
      "Loss gain: 0.06%\n",
      "Loss:  1702.3368\n",
      "Loss gain: 0.05%\n",
      "Loss:  1702.1917\n",
      "Loss gain: 0.04%\n",
      "Loss:  1702.0294\n",
      "Loss gain: 0.04%\n",
      "Loss:  1701.9031\n",
      "Loss gain: 0.04%\n",
      "Loss:  1701.7683\n",
      "Loss gain: 0.03%\n",
      "Loss:  1701.6635\n",
      "Loss gain: 0.03%\n",
      "Loss:  1701.5215\n",
      "Loss gain: 0.03%\n",
      "Loss:  1701.3796\n",
      "Loss gain: 0.03%\n",
      "Loss:  1701.2863\n",
      "Loss gain: 0.03%\n",
      "Loss:  1701.1938\n",
      "Loss gain: 0.03%\n",
      "Loss:  1701.0496\n",
      "Loss gain: 0.03%\n",
      "Loss:  1700.9504\n",
      "Loss gain: 0.03%\n",
      "Loss:  1700.8422\n",
      "Loss gain: 0.03%\n",
      "Loss:  1700.7478\n",
      "Loss gain: 0.03%\n",
      "Loss:  1700.6373\n",
      "Loss gain: 0.02%\n",
      "Loss:  1700.5271\n",
      "Loss gain: 0.02%\n",
      "Loss:  1700.422\n",
      "Loss gain: 0.02%\n",
      "Loss:  1700.3387\n",
      "Loss gain: 0.02%\n",
      "Loss:  1700.2301\n",
      "Loss gain: 0.02%\n",
      "Loss:  1700.0984\n",
      "Loss gain: 0.03%\n",
      "Loss:  1700.0088\n",
      "Loss gain: 0.02%\n",
      "Loss:  1699.9076\n",
      "Loss gain: 0.03%\n",
      "Loss:  1699.8213\n",
      "Loss gain: 0.02%\n",
      "Loss:  1699.7223\n",
      "Loss gain: 0.02%\n",
      "Loss:  1699.6162\n",
      "Loss gain: 0.02%\n",
      "Loss:  1699.5215\n",
      "Loss gain: 0.02%\n",
      "Loss:  1699.4114\n",
      "Loss gain: 0.02%\n",
      "Loss:  1699.3284\n",
      "Loss gain: 0.02%\n",
      "Loss:  1699.2352\n",
      "Loss gain: 0.02%\n",
      "Loss:  1699.1373\n",
      "Loss gain: 0.02%\n",
      "Loss:  1699.0562\n",
      "Loss gain: 0.02%\n",
      "Loss:  1698.9463\n",
      "Loss gain: 0.02%\n",
      "Loss:  1698.8435\n",
      "Loss gain: 0.02%\n",
      "Loss:  1698.7673\n",
      "Loss gain: 0.02%\n",
      "Loss:  1698.679\n",
      "Loss gain: 0.02%\n",
      "Loss:  1698.6075\n",
      "Loss gain: 0.02%\n",
      "Loss:  1698.4849\n",
      "Loss gain: 0.02%\n",
      "Loss:  1698.4012\n",
      "Loss gain: 0.02%\n",
      "Loss:  1698.3484\n",
      "Loss gain: 0.02%\n",
      "Loss:  1698.2313\n",
      "Loss gain: 0.02%\n",
      "Loss:  1698.1301\n",
      "Loss gain: 0.02%\n",
      "Loss:  1698.0679\n",
      "Loss gain: 0.02%\n",
      "Loss:  1697.9741\n",
      "Loss gain: 0.02%\n",
      "Loss:  1697.9205\n",
      "Loss gain: 0.02%\n",
      "Loss:  1697.8268\n",
      "Loss gain: 0.02%\n",
      "Loss:  1697.7664\n",
      "Loss gain: 0.02%\n",
      "Loss:  1697.696\n",
      "Loss gain: 0.02%\n",
      "Loss:  1697.6025\n",
      "Loss gain: 0.02%\n",
      "Loss:  1697.5444\n",
      "Loss gain: 0.02%\n",
      "Loss:  1697.4777\n",
      "Loss gain: 0.02%\n",
      "Loss:  1697.4003\n",
      "Loss gain: 0.02%\n",
      "Loss:  1697.3462\n",
      "Loss gain: 0.02%\n",
      "Loss:  1697.2639\n",
      "Loss gain: 0.02%\n",
      "Loss:  1697.2213\n",
      "Loss gain: 0.02%\n",
      "Loss:  1697.1534\n",
      "Loss gain: 0.01%\n",
      "Loss:  1697.0981\n",
      "Loss gain: 0.01%\n",
      "Loss:  1697.0458\n",
      "Loss gain: 0.01%\n",
      "Loss:  1697.0024\n",
      "Loss gain: 0.01%\n",
      "Loss:  1696.9363\n",
      "Loss gain: 0.01%\n",
      "Loss:  1696.9088\n",
      "Loss gain: 0.01%\n",
      "Loss:  1696.8268\n",
      "Loss gain: 0.01%\n",
      "Loss:  1696.7916\n",
      "Loss gain: 0.01%\n",
      "Loss:  1696.7777\n",
      "Loss gain: 0.01%\n",
      "Loss:  6530.557\n",
      "Loss:  4027.6533\n",
      "Loss gain: 38.33%\n",
      "Loss:  2688.377\n",
      "Loss gain: 58.83%\n",
      "Loss:  2118.7285\n",
      "Loss gain: 67.56%\n",
      "Loss:  1950.8284\n",
      "Loss gain: 70.13%\n",
      "Loss:  1878.3435\n",
      "Loss gain: 53.36%\n",
      "Loss:  1837.1821\n",
      "Loss gain: 31.66%\n",
      "Loss:  1810.5616\n",
      "Loss gain: 14.54%\n",
      "Loss:  1792.1803\n",
      "Loss gain: 8.13%\n",
      "Loss:  1779.0474\n",
      "Loss gain: 5.29%\n",
      "Loss:  1769.3062\n",
      "Loss gain: 3.69%\n",
      "Loss:  1761.7548\n",
      "Loss gain: 2.7%\n",
      "Loss:  1755.5635\n",
      "Loss gain: 2.04%\n",
      "Loss:  1750.3444\n",
      "Loss gain: 1.61%\n",
      "Loss:  1745.8074\n",
      "Loss gain: 1.33%\n",
      "Loss:  1741.7866\n",
      "Loss gain: 1.13%\n",
      "Loss:  1738.1537\n",
      "Loss gain: 0.99%\n",
      "Loss:  1734.8687\n",
      "Loss gain: 0.88%\n",
      "Loss:  1731.8826\n",
      "Loss gain: 0.8%\n",
      "Loss:  1729.1592\n",
      "Loss gain: 0.72%\n",
      "Loss:  1726.6342\n",
      "Loss gain: 0.66%\n",
      "Loss:  1724.2719\n",
      "Loss gain: 0.61%\n",
      "Loss:  1722.0233\n",
      "Loss gain: 0.57%\n",
      "Loss:  1719.8962\n",
      "Loss gain: 0.54%\n",
      "Loss:  1717.9156\n",
      "Loss gain: 0.5%\n",
      "Loss:  1716.0609\n",
      "Loss gain: 0.48%\n",
      "Loss:  1714.3595\n",
      "Loss gain: 0.45%\n",
      "Loss:  1712.8308\n",
      "Loss gain: 0.41%\n",
      "Loss:  1711.4126\n",
      "Loss gain: 0.38%\n",
      "Loss:  1710.1846\n",
      "Loss gain: 0.34%\n",
      "Loss:  1709.0278\n",
      "Loss gain: 0.31%\n",
      "Loss:  1708.0209\n",
      "Loss gain: 0.28%\n",
      "Loss:  1707.0944\n",
      "Loss gain: 0.25%\n",
      "Loss:  1706.2516\n",
      "Loss gain: 0.23%\n",
      "Loss:  1705.4905\n",
      "Loss gain: 0.21%\n",
      "Loss:  1704.746\n",
      "Loss gain: 0.19%\n",
      "Loss:  1704.0848\n",
      "Loss gain: 0.18%\n",
      "Loss:  1703.4442\n",
      "Loss gain: 0.16%\n",
      "Loss:  1702.8718\n",
      "Loss gain: 0.15%\n",
      "Loss:  1702.3116\n",
      "Loss gain: 0.14%\n",
      "Loss:  1701.7872\n",
      "Loss gain: 0.13%\n",
      "Loss:  1701.2926\n",
      "Loss gain: 0.13%\n",
      "Loss:  1700.8586\n",
      "Loss gain: 0.12%\n",
      "Loss:  1700.4419\n",
      "Loss gain: 0.11%\n",
      "Loss:  1700.0754\n",
      "Loss gain: 0.1%\n",
      "Loss:  1699.6808\n",
      "Loss gain: 0.09%\n",
      "Loss:  1699.3724\n",
      "Loss gain: 0.09%\n",
      "Loss:  1699.0145\n",
      "Loss gain: 0.08%\n",
      "Loss:  1698.7333\n",
      "Loss gain: 0.08%\n",
      "Loss:  1698.4138\n",
      "Loss gain: 0.07%\n",
      "Loss:  1698.1926\n",
      "Loss gain: 0.07%\n",
      "Loss:  1697.928\n",
      "Loss gain: 0.06%\n",
      "Loss:  1697.7216\n",
      "Loss gain: 0.06%\n",
      "Loss:  1697.5205\n",
      "Loss gain: 0.05%\n",
      "Loss:  1697.3511\n",
      "Loss gain: 0.05%\n",
      "Loss:  1697.1592\n",
      "Loss gain: 0.05%\n",
      "Loss:  1697.0287\n",
      "Loss gain: 0.04%\n",
      "Loss:  1696.8785\n",
      "Loss gain: 0.04%\n",
      "Loss:  1696.7638\n",
      "Loss gain: 0.03%\n",
      "Loss:  1696.6127\n",
      "Loss gain: 0.03%\n",
      "Loss:  1696.5262\n",
      "Loss gain: 0.03%\n",
      "Loss:  1696.4198\n",
      "Loss gain: 0.03%\n",
      "Loss:  1696.3595\n",
      "Loss gain: 0.02%\n",
      "Loss:  1696.2365\n",
      "Loss gain: 0.02%\n",
      "Loss:  1696.1743\n",
      "Loss gain: 0.02%\n",
      "Loss:  1696.1107\n",
      "Loss gain: 0.02%\n",
      "Loss:  1696.0549\n",
      "Loss gain: 0.02%\n",
      "Loss:  1695.9921\n",
      "Loss gain: 0.01%\n",
      "Loss:  1695.9608\n",
      "Loss gain: 0.01%\n",
      "Loss:  1695.868\n",
      "Loss gain: 0.01%\n",
      "Loss:  1695.8402\n",
      "Loss gain: 0.01%\n",
      "Loss:  1695.7998\n",
      "Loss gain: 0.01%\n",
      "Loss:  1695.787\n",
      "Loss gain: 0.01%\n",
      "Loss:  1695.7437\n",
      "Loss gain: 0.01%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "vocabulary_size = len(vocabulary)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "net = Word2Vec(embedding_size=4, vocab_size=vocabulary_size)\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "early_stopping = EarlyStopping(patience=5, min_percent_gain=0.01)\n",
    "\n",
    "while True:\n",
    "    losses = []\n",
    "    context_tuple_batches = get_batches(context_tuple_list, batch_size=2000)\n",
    "    for i in range(len(context_tuple_batches)):\n",
    "        net.zero_grad()\n",
    "        target_tensor, context_tensor, negative_tensor = context_tuple_batches[i]\n",
    "        loss = net(target_tensor, context_tensor, negative_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.data)\n",
    "    print(\"Loss: \", np.mean(losses))\n",
    "    early_stopping.update_loss(np.mean(losses))\n",
    "    if early_stopping.stop_training():\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'word2vec.pth')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
